# ----------------------------------------------------------------------- #
# Basic options
# ----------------------------------------------------------------------- #

# Set print destination: stdout / file / both
# print: both

# Select device: 'cpu', 'cuda', 'auto'
# accelerator:

# number of devices: eg. for 2 GPU set cfg.devices=2
# devices = 1

# Output directory
# out_dir = results

# Random seed
# seed = 0

# Print rounding
# round = 4

# Max threads used by PyTorch
# num_threads = 6

# ----------------------------------------------------------------------- #
# Dataset options
# dataset:
#   configs...
# ----------------------------------------------------------------------- #

# Name of the dataset
# name = 'Cora'

# if PyG: look for it in Pytorch Geometric dataset
# if NetworkX/nx: load data in NetworkX format
# OGB
# format = PyG

# Dir to load the dataset. If the dataset is downloaded, this is the cache dir
# dir = ./datasets

# Task: node, edge, graph, link_pred
# task = node

# Type of task: classification, regression, classification_binary or classification_multi
# task_type = classification

# Transductive / Inductive
# Tranductive gather all data in training and testing to predict the new ones
# Inductive only sees training and try to predict testing data
# Graph classification is always inductive
# transductive = True

# Split ratio of dataset. Len=2: Train, Val. Len=3: Train, Val, Test
# split = [0.8, 0.1, 0.1]

# Whether to shuffle the graphs for splitting
# shuffle_split = True

# Whether random split or use custom split: random / custom
# split_mode = 'random'

# Whether to use an encoder for general attribute features
# encoder = True

# Name of general encoder
# encoder_name = 'db'

# If add batchnorm after general encoder
# encoder_bn = True

# Whether to use an encoder for the node features
# node_encoder = False

# Name of node encoder
# node_encoder_name = 'Atom'

# If add batchnorm after node encoder
# node_encoder_bn = True

# Whether to use an encoder for the edge features
# edge_encoder = False

# Name of edge encoder
# edge_encoder_name = 'Bond'

# If add batchnorm after edge encoder
# edge_encoder_bn = True

# Dimension of the encoded features. For now the node and edge encoding dimensions are the same.
# encoder_dim = 128

# Dimension for edge feature. Updated by the real dim of the dataset
# edge_dim = 128

# ============== Link/edge tasks only

# all or disjoint
# edge_train_mode = 'all'

# Used in disjoint edge_train_mode. The proportion of edges used for
# message-passing
# edge_message_ratio = 0.8

# The ratio of negative samples to positive samples
# edge_negative_sampling_ratio = 1.0

# Whether resample disjoint when dataset.edge_train_mode is 'disjoint'
# resample_disjoint = False

# Whether resample negative edges at training time (link prediction only)
# resample_negative = False

# What transformation function is applied to the dataset
# transform = 'none'

# Whether cache the splitted dataset
# NOTE: it should be cautiouslly used, as cached dataset may not have
# exactly the same setting as the config file
# cache_save = False
# cache_load = False

# Whether remove the original node features in the dataset
# remove_feature = False

# Simplify TU dataset for synthetic tasks
# tu_simple = True

# Convert to undirected graph (save 2*E edges)
# to_undirected = False

# dataset location: local, snowflake
# location = 'local'

# Define label: Table name
# label_table = 'none'

# Define label: Column name
# label_column = 'none'

# ----------------------------------------------------------------------- #
# Training options
# train:
#   configs...
# ----------------------------------------------------------------------- #

# Total graph mini-batch size
# batch_size = 16

# Sampling strategy for a train loader
# sampler = 'full_batch'

# Minibatch node
# sample_node = False

# Num of sampled node per graph
# node_per_graph = 32

# Radius: same, extend. same: same # layers_mp, extend: layers+1
# radius = 'extend'

# Evaluate model on test data every eval period epochs
# eval_period = 10

# Option to skip training epoch evaluation
# skip_train_eval = False

# Save model checkpoint every checkpoint period epochs
# ckpt_period = 100

# Enabling checkpoint, set False to disable and save I/O
# enable_ckpt = True

# Resume training from the latest checkpoint in the output directory
# auto_resume = False

# The epoch to resume. -1 means resume the latest epoch.
# epoch_resume = -1

# Clean checkpoint: only keep the last ckpt
# ckpt_clean = True

# Number of iterations per epoch (for sampling based loaders only)
# iter_per_epoch = 32

# GraphSAINTRandomWalkSampler: random walk length
# walk_length = 4

# NeighborSampler: number of sampled nodes per layer
# neighbor_sizes = [20, 15, 10, 5]

# ----------------------------------------------------------------------- #
# Validation options
# val:
#   configs...
# ----------------------------------------------------------------------- #

# Minibatch node
# sample_node = False

# Sampling strategy for a val/test loader
# sampler = 'full_batch'

# Num of sampled node per graph
# node_per_graph = 32

# Radius: same, extend. same: same # layers_mp, extend: layers+1
# radius = 'extend'

# ----------------------------------------------------------------------- #
# Model options
# model:
#   configs...
# ----------------------------------------------------------------------- #

# Model type to use
# type = 'gnn'

# Auto match computational budget, match upper bound / lower bound
# match_upper = True

# Loss function: cross_entropy, mse
# loss_fun = 'cross_entropy'

# size average for loss function. 'mean' or 'sum'
# size_average = 'mean'

# Threshold for binary classification
# thresh = 0.5

# ============== Link/edge tasks only
# Edge decoding methods.
#   - dot: compute dot(u, v) to predict link (binary)
#   - cosine_similarity: use cosine similarity (u, v) to predict link (
#   binary)
#   - concat: use u||v followed by an nn.Linear to obtain edge embedding
#   (multi-class)
# edge_decoding = 'dot'
# ===================================

# ================== Graph tasks only
# Pooling methods.
#   - add: global add pool
#   - mean: global mean pool
#   - max: global max pool
# graph_pooling = 'add'
# ===================================

# ----------------------------------------------------------------------- #
# GNN options
# gnn:
#   configs...
# ----------------------------------------------------------------------- #

# Prediction head. Use# task by default
# head = 'default'

# Number of layers before message passing
# layers_pre_mp = 0

# Number of layers for message passing
# layers_mp = 2

# Number of layers after message passing
# layers_post_mp = 0

# Hidden layer dim. Automatically set if train.auto_match = True
# dim_inner = 16

# Type of graph conv: generalconv, gcnconv, sageconv, gatconv, ...
# layer_type = 'generalconv'

# Stage type: 'stack', 'skipsum', 'skipconcat'
# stage_type = 'stack'

# How many layers to skip each time
# skip_every = 1

# Whether use batch norm
# batchnorm = True

# Activation
# act = 'relu'

# Dropout
# dropout = 0.0

# Aggregation type: add, mean, max
# Note: only for certain layers that explicitly set aggregation type
# e.g., # layer_type = 'generalconv'
# agg = 'add'

# Normalize adj
# normalize_adj = False

# Message direction: single, both
# msg_direction = 'single'

# Whether add message from node itself: none, add, cat
# self_msg = 'concat'

# Number of attention heads
# att_heads = 1

# After concat attention heads, add a linear layer
# att_final_linear = False

# After concat attention heads, add a linear layer
# att_final_linear_bn = False

# Normalize after message passing
# l2norm = True

# randomly use fewer edges for message passing
# keep_edge = 0.5

# clear cached feature_new
# clear_feature = True

# ----------------------------------------------------------------------- #
# Optimizer options
# optim:
#   configs...
# ----------------------------------------------------------------------- #

# optimizer: sgd, adam
# optimizer = 'adam'

# Base learning rate
# base_lr = 0.01

# L2 regularization
# weight_decay = 5e-4

# SGD momentum
# momentum = 0.9

# scheduler: none, steps, cos
# scheduler = 'cos'

# Steps for 'steps' policy (in epochs)
# steps = [30, 60, 90]

# Learning rate multiplier for 'steps' policy
# lr_decay = 0.1

# Maximal number of epochs
# max_epoch = 200

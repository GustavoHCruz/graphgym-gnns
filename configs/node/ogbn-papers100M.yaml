# ----------------------------------------------------------------------- #
# Basic options
# ----------------------------------------------------------------------- #

# Set print destination: stdout, file, both
print: both

# Select device: 'cpu', 'cuda', 'auto'
accelerator: cuda

# number of devices
devices: 1

# Output directory
out_dir: results

# Random seed
seed: 0

# Print rounding
round: 4

# Max threads used by PyTorch
num_threads: 12

# ----------------------------------------------------------------------- #
# Dataset options
# ----------------------------------------------------------------------- #
dataset:
  # Name of the dataset
  name: ogbn-papers100M

  # Formats: PyG, NetworkX/nx, OGB
  format: OGB

  # Dir to load the dataset
  dir: ./datasets

  # Task: node, edge, graph, link_pred
  task: node

  # Type of task: classification, regression, classification_binary, classification_multi
  task_type: classification

  # Transductive, Inductive
  transductive: true

  # Split ratio of dataset
  split: [0.8, 0.1, 0.1]

  # Whether to shuffle the graphs for splitting
  shuffle_split: True

  # Whether random split or use custom split: random / custom
  split_mode: random

  # Whether to use an encoder for general attribute features
  encoder: false

  # Name of general encoder
  # encoder_name: paper

  # If add batchnorm after general encoder
  encoder_bn: false

  # Whether to use an encoder for the node features
  node_encoder: false

  # Name of node encoder
  # node_encoder_name: Atom

  # If add batchnorm after node encoder
  node_encoder_bn: false

  # Whether to use an encoder for the edge features
  edge_encoder: false

  # Name of edge encoder
  # edge_encoder_name: 'Bond'

  # If add batchnorm after edge encoder
  edge_encoder_bn: false

  # Dimension of the encoded features
  encoder_dim: 16

  # Dimension for edge feature. Updated by the real dim of the dataset
  edge_dim: 16

train:
  # Total graph mini-batch size
  batch_size: 2

  # Sampling strategy for a train loader
  sampler: full_batch

  # Minibatch node
  sample_node: false

  # Num of sampled node per graph
  node_per_graph: 4

  # Option to skip training epoch evaluation
  skip_train_eval: false

  # Evaluate model on test data every eval period epochs
  eval_period: 20

  # Enabling checkpoint, set False to disable and save I/O
  enable_ckpt: false

  # Save model checkpoint every checkpoint period epochs
  # ckpt_period: 100

  # Resume training from the latest checkpoint in the output directory
  auto_resume: false

  # The epoch to resume. -1 means resume the latest epoch.
  # epoch_resume: -1

  # Clean checkpoint: only keep the last ckpt
  ckpt_clean: true

model:
  # Model type to use
  type: gnn

  # Auto match computational budget, match upper bound / lower bound
  match_upper: true

  # Loss function: cross_entropy, mse
  loss_fun: cross_entropy

  # Size average: mean, sum
  size_average: mean

  # Threshold for binary classification
  thresh: 0.5

gnn:
  # Prediction head. Use # task by default
  head: default

  # Number of layers before message passing
  layers_pre_mp: 0

  # Number of layers for message passing
  layers_mp: 2

  # Number of layers after message passing
  layers_post_mp: 1

  # Hidden layer dim. Automatically set if train.auto_match = True
  dim_inner: 16

  # Type of graph conv: generalconv, gcnconv, sageconv, gatconv, ...
  layer_type: generalconv

  # Stage type: 'stack', 'skipsum', 'skipconcat'
  stage_type: stack

  # How many layers to skip each time
  skip_every: 1

  # Whether use batch norm
  batchnorm: true

  # Activation
  act: relu

  # Dropout
  dropout: 0.0

  # Aggregation type: add, mean, max
  agg: add

  # Normalize adj
  normalize_adj: false

  # Message direction: single, both
  # msg_direction: single

  # Whether add message from node itself: none, add, cat
  # self_msg: concat

  # Number of attention heads
  att_heads: 1

  # After concat attention heads, add a linear layer
  att_final_linear: false

  # Normalize after message passing
  # l2norm: true

  # randomly use fewer edges for message passing
  # keep_edge: 0.5

  # clear cached feature_new
  clear_feature: true

optim:
  # optimizer: sgd, adam
  optimizer: adam

  # Base learning rate
  base_lr: 0.01

  # L2 regularization
  weight_decay: 5e-4

  # SGD momentum
  # momentum: 0.9

  # Maximal number of epochs
  max_epoch: 10
